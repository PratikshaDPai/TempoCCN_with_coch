# TempoCCN_with_coch
USAGE OF NEURAL ACTIVITY PATTERN GENERATED COCHLEAGRAM FOR TEMPO ESTIMATION WITH A CNN

1.	Background/ Description:

Tempo estimation is the process of determining the number of beats per minute in an audio signal , usually measuring the frequency at which humans ‘tap’ to the beat of a piece. It has many applications in Signal Processing and Multimedia domains.
Octave confusion is a challenging problem that impedes tempo estimation, wherein the tempo of an audio piece is confused with an integral multiple. This is also faced in tempo estimation by a human listener, which is why integer multiples of 2 or 3 times the original tempo are also acceptable.
The current state of the art uses Mel- Spectrograms with a classification learning based Convolutional Neural Network (CNN) for local tempo estimation. Local tempo is averaged to find global tempo, with best (till date) performance with respect to octave confusion[1]
This project aims to improve the performance of the current state of the art, by using Cochleagram inputs, derived from the Neural Activity Pattern generated by a model of the human auditory system.


Cochleagram versus Mel Spectrogram
Frequency is downsampled  into frequency bands with equivalent rectangular bandwidth (ERB) scale. Basically, the main difference between spectrogram and cochleogram is that cochleogram features are based on ERB scale that has finer resolution at low frequencies than the mel scale used in spectrogram.
After we define the combination of the gammatone filterbank, we apply the filterbank into the ACM-Mirium dataset to generate a cochleogram which represents transformed audio input  in time and frequency domain. 

 Description of different code sections:
 
 demo_aim2006_dcgc_one.m : used to plot different cochleagream outputs for wav audio inputs. 
 

